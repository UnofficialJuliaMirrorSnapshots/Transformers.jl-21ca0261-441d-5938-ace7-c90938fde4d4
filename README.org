* Transformers.jl
Julia implementation of NLP models, that based on google [[https://arxiv.org/abs/1706.03762][transformer]], with [[https://github.com/FluxML/Flux.jl][Flux.jl]].
For using the model, see =example= folder.

* Installation
In the Julia REPL:
#+BEGIN_EXAMPLE
]add Transformers

#Currently the Dataset need the HTTP#master to download WMT
]add HTTP#master
#+END_EXAMPLE

For using GPU, install & build:
#+BEGIN_EXAMPLE
]add CuArrays

]build 

julia> using CuArrays

julia> using Transformers

#run the model below
.
.
.

#+END_EXAMPLE

* Implemented model
You can find the code in =example= folder.

+ [[https://arxiv.org/abs/1706.03762][Attention is all you need]]
+ [[https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf][Improving Language Understanding by Generative Pre-Training]]

* Example
Take a simple encoder-decoder model construction for machine translation task. With =Transformers.jl= we can easily define/stack the models. 

#+BEGIN_SRC julia
using Transformers
using Transformers.Basic

encoder = Stack(
    @nntopo(e → pe:(e, pe) → x → x → $N),
    PositionEmbedding(512),
    (e, pe) -> e .+ pe,
    Dropout(0.1),
    [Transformer(512, 8, 64, 2048) for i = 1:N]...
)

decoder = Stack(
    @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c),
    PositionEmbedding(512),
    (e, pe) -> e .+ pe,
    Dropout(0.1),
    [TransformerDecoder(512, 8, 64, 2048) for i = 1:N]...,
    Positionwise(Dense(512, length(labels)), logsoftmax)
)

function loss(src, trg, src_mask, trg_mask)
    label = onehot(vocab, trg)

    src = embedding(src)
    trg = embedding(trg)

    mask = getmask(src_mask, trg_mask)

    enc = encoder(src)
    dec = decoder(trg, enc, mask)

    loss = logkldivergence(label, dec[:, 1:end-1, :], trg_mask[:, 1:end-1, :])
end
#+END_SRC

See =example= folder for the complete example.

* Usage
** Transformer
   The =Transformer= and =TransformerDecoder= is the encoder and decoder block of the origin paper, and they are all implement as the 
   regular Flux Layer, so you can treat them just as the =Dense= layer. See the docstring for the argument. However, for the sequence 
   data input, we usually have a 3 dimensional input of shape =(hidden size, sequence length, batch size)= instead of just =(hidden size, batch size)=. 
   Therefore, we implement both 2d & 3d operation according to the input type (The =N= of =Array{T, N}=). We are able to handle both input of shape 
   =(hidden size, sequence length, batch size)= and =(hidden size, sequence length)= for the case with only 1 input.

#+BEGIN_SRC julia
using Transfomers

m = Transformer(512, 8, 64, 2048) #define a Transformer block with 8 head and 64 neuron for each head
x = randn(512, 30, 3) #fake data of length 30

y = m(x)
#+END_SRC

** Positionwise
   For the sequential task, we need to handle the 3 dimensional input. However, most of the layer in Flux only support input with shape 
   =(hidden size, batch size)=. In order to tackle this problem, we implement the =Positionwise= helper function that is almost the same 
   as =Flux.Chain= but it will run the model position-wisely. (internally it just reshape the input to 2d and apply the model then reshape 
   back). 

#+BEGIN_SRC julia
using Transformers
using Flux

m = Positionwise(Dense(10, 5), Dense(5, 2), softmax)
x = randn(10, 30, 3)

y = m(x)

# which is equivalent to 
# 
# m = Chain(Dense(10, 5), Dense(5, 2), softmax)
# x1 = randn(10, 30)
# x2 = randn(10, 30)
# x3 = randn(10, 30)
# y = cat(m(x1), m(x2), m(x3); dims=3)

#+END_SRC

** The Stack NNTopo DSL
   Since the =TransformerDecoder= require more than one input, it's not convenient to use with =Chain=. Therefore, we implement a very simple 
   DSL(Domain Specific Language) to handle the function structure. You can use the =@nntopo= macro to define the structure then call the function 
   with the given model.

*** NNTopo Syntax
    we call the DSL NNTopo for "Neural Network Topology", but actually it is just used to define where the input & output should be in a sequence of 
    function, or the complex version of the =|>= function in Julia.

**** "Chain" the functions
     For example:

#+BEGIN_SRC julia
y = h(f(g(x))) #a chain of function call

# or 
a = g(x)
b = f(a)
y = h(b)

# is equivalent to 
topo = @nntopo x => a => b => y # first we define the topology/architecture
y = topo((g, f, h), x) #then call on the given functions

#+END_SRC

    each ==>= is a function call, left hand side is the input argument and right hand side is the output name.

**** Loop unrolling
     you can also unroll a loop:

#+BEGIN_SRC julia
y = g(f(f(f(f(x)))))

# or 
tmp = x
for i = 1:4
  tmp = f(tmp)
end
y = g(tmp)

# is equivalent to 
topo = @nntopo x => 4 => y
y = topo((f,f,f,f, g), x) # f can also be different

#+END_SRC

**** Multiple argument & jump connection
     As we metioned above, the original intention was to handle the case that we have more than one input & output. So, we can do this with the following syntax: 

#+BEGIN_SRC julia
# a complex structure
# x1 to x4 in the given inputs
t = f(x1, x2)
z1, z2 = g(t, x3)
w = h(x4, z1)
y = k(x2, z2, w)

# is equivalent to 
topo = @nntopo (x1, x2, x3, x4):(x1, x2) => t:(t, x3) => (z1, z2):(x4, z1) => w:(x2, z2, w) => y
y = topo((f, g, h, k), x1, x2, x3, x4)

# you can also see the function with `print_topo` function
using Transformers.Basic: print_topo

print_topo(topo; models=(f, g, h, k))
# 
# NNTopo{"(x1, x2, x3, x4):(x1, x2) => (t:(t, x3) => ((z1, z2):(x4, z1) => (w:(x2, z2, w) => y)))"}
# topo_func(model, x1, x2, x3, x4)
#         t = f(x1, x2)
#         (z1, z2) = g(t, x3)
#         w = h(x4, z1)
#         y = k(x2, z2, w)
#         y
# end

#+END_SRC

**** Specify the variables you want
    Notice that we use a =:= to seperate the input/output variables name for each function call, if the =:= is not present, we will by default assume 
    the output variables are all the inputs of the next function call. i.e. =x => (t1, t2) => y= is equal to =x => (t1, t2):(t1, t2) => y=. 

    We can also return multiple variables, so the complete syntax can be viewed as:

#+BEGIN_EXAMPLE
(input arguments):(function1 inputs) => (function1 outputs):(function2 inputs):(function2 outputs) => .... => (function_n outputs):(return variables)
#+END_EXAMPLE 

**** Interpolation
     we also support interpolation, so you can use a variable to hold a substructure or the unroll number. But *notice* that the 
     interpolation variable should always be at the top level of the module since we can only get that value with =eval=

#+BEGIN_SRC julia
N = 3
topo = @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c)

print_topo(topo)
# 
# NNTopo{"(e, m, mask):e → (pe:(e, pe) → (t → ((t:(t, m, mask) → t:(t, m, mask)) → (3:t → c))))"}
# topo_func(model, e, m, mask)
#         pe = model[1](e)
#         t = model[2](e, pe)
#         t = model[3](t)
#         t = model[4](t, m, mask)
#         t = model[5](t, m, mask)
#         t = model[6](t, m, mask)
#         c = model[7](t)
#         c
# end

#+END_SRC

**** Nested Structure
     you can also use the =()= to create a nested structure for the unroll.

#+BEGIN_SRC julia
topo = @nntopo x => ((y => z => t) => 3 => w) => 2
print_topo(topo)
# 
# NNTopo{"x => (((y => (z => t)) => (3 => w)) => 2)"}
# topo_func(model, x)
#         y = model[1](x)
#         z = model[2](y)
#         t = model[3](z)
#         z = model[4](t)
#         t = model[5](z)
#         z = model[6](t)
#         t = model[7](z)
#         w = model[8](t)
#         z = model[9](w)
#         t = model[10](z)
#         z = model[11](t)
#         t = model[12](z)
#         z = model[13](t)
#         t = model[14](z)
#         w = model[15](t)
#         w
# end

#+END_SRC

*** Stack
    With the NNTopo DSL, now we can simple use the NNTopo with our Stack type, which is also like the =Chain= but we also need to pass in the 
    =topo= for the architecture.

#+BEGIN_SRC julia
#The Decoder Example in Attention is All you need
Stack(
    @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c),
    PositionEmbedding(512),
    (e, pe) -> e .+ pe,
    Dropout(0.1),
    [TransformerDecoder(512, 8, 64, 2048) for i = 1:N]...,
    Positionwise(Dense(512, length(labels)), logsoftmax)
)
#+END_SRC

* Issue
Currently the code only work without problems on linux. For Mac/Windows, it will show a large amount of warning. Unfortunately, I haven't figure out what 
is wrong and I don't have a Mac/Windows machine to test, so if you happend to know how to handle it, Please fire an Issue/PR.

* Roadmap
  - [33%] write docs
    - [X] docstring
    - [ ] examples
    - [ ] make docs site
  - [X] write test
  - [ ] refactor code
  - [50%] better embedding functions
    - [X] gather function forward
    - [X] gather function backward (might be better)
    - [X] OneHotArray
    - [ ] more util functions
    - [ ] easy gpu data
    - [ ] remove Vocabulary
  - [X] lazy CuArrays loading
  - [ ] using HTTP to handle dataset download (need HTTP.jl update)
  - [ ] optimize performance
  - [ ] text related util functions
  - [ ] better dataset API
  - [ ] more datasets
  - [75%] openai gpt model
    - [X] model implementation
    - [X] loading pretrain
    - [X] model example
    - [ ] more util functions
  - [ ] openai gpt-2 model
  - [ ] google bert model
  - [ ] TPU support
  - [ ] openai sparse transformer
